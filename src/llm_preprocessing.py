"""
LLM-based –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ main_pipeline.py —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥ --llm-clean

–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:
1. –£–¥–∞–ª—è–µ—Ç –º—É—Å–æ—Ä (–Ω–∞–≤–∏–≥–∞—Ü–∏—è, —Ñ—É—Ç–µ—Ä—ã, —Ä–µ–∫–ª–∞–º–∞) —á–µ—Ä–µ–∑ LLM
2. –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ (–ø—Ä–æ–¥—É–∫—Ç—ã, —É—Å–ª—É–≥–∏, —Ç–µ—Ä–º–∏–Ω—ã)
3. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
4. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å
5. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
"""
import pandas as pd
from llama_cpp import Llama
from tqdm import tqdm
import json
import re
from pathlib import Path
from typing import Dict, Optional

from src.config import (
    LLM_MODEL_FILE,
    LLM_CONTEXT_SIZE,
    LLM_GPU_LAYERS,
    MODELS_DIR
)


class LLMDocumentCleaner:
    """
    LLM-based –æ—á–∏—Å—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen3-32B (–∏–ª–∏ –¥—Ä—É–≥—É—é LLM) –¥–ª—è:
    - –£–¥–∞–ª–µ–Ω–∏—è –º—É—Å–æ—Ä–∞ –∏–∑ –≤–µ–±-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    - –î–æ–±–∞–≤–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    """

    def __init__(self, model_path: Optional[str] = None, verbose: bool = True):
        """
        Args:
            model_path: –ø—É—Ç—å –∫ GGUF –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑ config)
            verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        """
        if model_path is None:
            model_path = str(MODELS_DIR / LLM_MODEL_FILE)

        self.model_path = model_path
        self.verbose = verbose
        self.llm = None

        if verbose:
            print(f"\n{'='*80}")
            print(f"üì• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Document Cleaner")
            print(f"   –ú–æ–¥–µ–ª—å: {Path(model_path).name}")
            print(f"{'='*80}\n")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ LLM –º–æ–¥–µ–ª–∏"""
        if self.llm is not None:
            return  # —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞

        model_path = Path(self.model_path)

        if not model_path.exists():
            raise FileNotFoundError(
                f"LLM –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\n"
                f"–°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: python download_models.py"
            )

        if self.verbose:
            print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")

        self.llm = Llama(
            model_path=str(model_path),
            n_ctx=LLM_CONTEXT_SIZE,
            n_gpu_layers=LLM_GPU_LAYERS,
            n_batch=512,
            n_threads=8,
            use_mlock=True,
            verbose=False
        )

        if self.verbose:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def clean_document(self, text: str) -> Dict:
        """
        –û—á–∏—Å—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ LLM

        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            dict —Å –ø–æ–ª—è–º–∏:
                - clean_text: –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                - products: —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
                - actions: —Å–ø–∏—Å–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π
                - conditions: —Å–ø–∏—Å–æ–∫ —É—Å–ª–æ–≤–∏–π
                - topics: —Å–ø–∏—Å–æ–∫ —Ç–µ–º
                - usefulness_score: –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (0-1)
                - is_useful: bool
        """
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if self.llm is None:
            self.load_model()

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        text_truncated = text[:4000]

        prompt = f"""–¢—ã ‚Äî –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π –∏ —Å—Ç—Ä–æ–≥–∏–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫–∞. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ. 
–ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏ –ù–ï –¥–æ–±–∞–≤–ª—è–π –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ.

–¢–ï–ë–ï –î–ê–ù –ò–°–•–û–î–ù–´–ô –¢–ï–ö–°–¢ (–î–û–ö–£–ú–ï–ù–¢):
{text_truncated}

–í–´–ü–û–õ–ù–ò –°–õ–ï–î–£–Æ–©–ï–ï:

1. –û–ß–ò–°–¢–ö–ê –î–û–ö–£–ú–ï–ù–¢–ê
–£–¥–∞–ª–∏—Ç—å –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—Å—ë, —á—Ç–æ –Ω–µ –Ω–µ—Å—ë—Ç —Å–º—ã—Å–ª–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞:
- –Ω–∞–≤–∏–≥–∞—Ü–∏—é (–º–µ–Ω—é, —Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏, ¬´–ù–∞–∑–∞–¥¬ª, ¬´–ü–æ–¥–µ–ª–∏—Ç—å—Å—è¬ª, ¬´–í–≤–µ—Ä—Ö¬ª, –ø—É–Ω–∫—Ç—ã —Å–∞–π—Ç–∞),
- —Ñ—É—Ç–µ—Ä—ã (¬© 2001‚Äì2025, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∞–¥—Ä–µ—Å–∞ –æ—Ñ–∏—Å–æ–≤, –ª–∏—Ü–µ–Ω–∑–∏–∏, –∫–æ–ø–∏—Ä–∞–π—Ç—ã),
- —Ä–µ–∫–ª–∞–º–Ω—ã–µ –ª–æ–∑—É–Ω–≥–∏ –∏ –±–∞–Ω–Ω–µ—Ä—ã (¬´–û—Ç–∫—Ä–æ–π—Ç–µ –∫–∞—Ä—Ç—É —Å–µ–≥–æ–¥–Ω—è!¬ª, ¬´–£–∑–Ω–∞–π—Ç–µ –±–æ–ª—å—à–µ¬ª, ¬´–û—Å—Ç–∞–≤—å—Ç–µ –∑–∞—è–≤–∫—É¬ª),
- cookie-–±–∞–Ω–Ω–µ—Ä—ã –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è,
- —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏ (¬´–Ω–∞–∂–º–∏—Ç–µ –∑–¥–µ—Å—å¬ª, ¬´–ø–µ—Ä–µ–π–¥–∏—Ç–µ –ø–æ —Å—Å—ã–ª–∫–µ¬ª, ¬´—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ¬ª),
- –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –æ–Ω–∏ —Å–∞–º–∏ –ø–æ —Å–µ–±–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —á–∞—Å—Ç—å—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–ª–∏ —É—Å–ª–æ–≤–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞,
- –ø—É—Å—Ç—ã–µ/–æ–±—â–∏–µ —Å–ø–∏—Å–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π –∏–ª–∏ –¥–µ—Ç–∞–ª–µ–π.

–°–û–•–†–ê–ù–ò:
- –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤/—É—Å–ª—É–≥ –∏ —É—Å–ª–æ–≤–∏–π,
- –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è,
- –≤–∞–∂–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∫–æ–º–∏—Å—Å–∏–∏, –ª–∏–º–∏—Ç—ã, —Å—Ä–æ–∫–∏, –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è),
- —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–º–æ–≥—É—Ç –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫–ª–∏–µ–Ω—Ç–∞.

–ù–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ ‚Äî –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è–π –∏—Å—Ö–æ–¥–Ω—É—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É, —Ç–æ–ª—å–∫–æ —É–±–∏—Ä–∞–π –º—É—Å–æ—Ä –∏ –¥—É–±–ª–∏.

2. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–õ–Æ–ß–ï–í–û–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò
–û–ø—Ä–µ–¥–µ–ª–∏, –∫–∞–∫–∞—è –ø–æ–ª–µ–∑–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:
- –ü—Ä–æ–¥—É–∫—Ç—ã/—É—Å–ª—É–≥–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞, –ê-–ö–ª—É–±, –∏–ø–æ—Ç–µ–∫–∞, –≤–∫–ª–∞–¥, –∫—Ä–µ–¥–∏—Ç, –¥–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∏ —Ç.–ø.).
- –î–µ–π—Å—Ç–≤–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä: –æ–ø–ª–∞—Ç–∞ –ñ–ö–•, –ø–µ—Ä–µ–≤–æ–¥, –æ—Ç–∫—Ä—ã—Ç–∏–µ —Å—á—ë—Ç–∞, –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã, –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—ã–ø–∏—Å–∫–∏).
- –£—Å–ª–æ–≤–∏—è (–∫–æ–º–∏—Å—Å–∏–∏, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, –ª–∏–º–∏—Ç—ã, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–ª–∏–µ–Ω—Ç—É, —Å—Ä–æ–∫–∏, –≤–∞–ª—é—Ç—ã, –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Å—É–º–º—ã –∏ —Ç.–¥.).

3. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ï–ú (–ú–ê–ö–°–ò–ú–£–ú 3)
–í—ã–±–µ—Ä–∏ –Ω–µ –±–æ–ª–µ–µ —Ç—Ä—ë—Ö —Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ –≤—Å–µ–≥–æ –æ–ø–∏—Å—ã–≤–∞—é—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç, —Ç–æ–ª—å–∫–æ –∏–∑ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞:

–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ_–∫–∞—Ä—Ç—ã, –¥–µ–±–µ—Ç–æ–≤—ã–µ_–∫–∞—Ä—Ç—ã, –ø–µ—Ä–µ–≤–æ–¥—ã, –∂–∫—Ö, –∫—ç—à–±—ç–∫,
—Å—á–µ—Ç–∞_—Ä–µ–∫–≤–∏–∑–∏—Ç—ã, –∫–æ–º–∏—Å—Å–∏–∏, –ª–∏–º–∏—Ç—ã, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å,
–º–æ–±–∏–ª—å–Ω–æ–µ_–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –∞–ª—å—Ñ–∞_–æ–Ω–ª–∞–π–Ω, –∏–ø–æ—Ç–µ–∫–∞, –∫—Ä–µ–¥–∏—Ç—ã,
–≤–∫–ª–∞–¥—ã, –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏, —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ

4. –û–¶–ï–ù–ö–ê –ü–û–õ–ï–ó–ù–û–°–¢–ò –î–û–ö–£–ú–ï–ù–¢–ê
–û—Ü–µ–Ω–∏, –Ω–∞—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∫–ª–∏–µ–Ω—Ç–∞ –±–∞–Ω–∫–∞:

- 0.0‚Äì0.3 ‚Äî –ø–æ—á—Ç–∏ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–π (–≤ –æ—Å–Ω–æ–≤–Ω–æ–º –Ω–∞–≤–∏–≥–∞—Ü–∏—è, —Ä–µ–∫–ª–∞–º–∞, –æ–±—â–∏–µ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ —Ñ—Ä–∞–∑—ã).
- 0.4‚Äì0.6 ‚Äî —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–π (–µ—Å—Ç—å –≤–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –Ω–æ –æ–Ω–∞ –æ–±—Ä—ã–≤–æ—á–Ω–∞—è –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è).
- 0.7‚Äì1.0 ‚Äî –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω—ã–π (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —Ç–∞—Ä–∏—Ñ—ã, —à–∞–≥–∏).

–ù–µ —Å—Ç–∞–≤—å –æ—Ü–µ–Ω–∫—É > 0.6, –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏ (—á–∏—Å–µ–ª, —á—ë—Ç–∫–∏—Ö —É—Å–ª–æ–≤–∏–π, –ø–æ–Ω—è—Ç–Ω—ã—Ö —à–∞–≥–æ–≤).

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê
–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:

{{
  "clean_text": "–æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞, –±–µ–∑ –º—É—Å–æ—Ä–∞",
  "key_info_summary": "–∫—Ä–∞—Ç–∫–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Ä–µ–∑—é–º–µ —Ç–æ–≥–æ, –æ —á—ë–º —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç (1‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
  "topics": ["—Ç–µ–º–∞_1", "—Ç–µ–º–∞_2"],
  "usefulness_score": 0.0,
  "reasoning": "–∫—Ä–∞—Ç–∫–æ –æ–±—ä—è—Å–Ω–∏, –ø–æ—á–µ–º—É –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –∏–º–µ–Ω–Ω–æ —Ç–∞–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)"
}}"""

        try:
            response = self.llm(
                prompt,
                max_tokens=2048,
                temperature=0.1,
                stop=["<|im_end|>"],
            )

            response_text = response['choices'][0]['text']

            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(0))
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–µ–π –ø–æ–¥ downstream:
                # –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏
                result.setdefault("clean_text", text_truncated)
                result.setdefault("topics", [])
                result.setdefault("usefulness_score", 0.5)
                # —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: —ç—Ç–∏ –ø–æ–ª—è –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è downstream (entities —Å–±–æ—Ä–∫–∞)
                result.setdefault("products", [])
                result.setdefault("actions", [])
                result.setdefault("conditions", [])
                # derive is_useful –ø–æ –ø—Ä–µ–∂–Ω–µ–π –ª–æ–≥–∏–∫–µ (–ø–æ—Ä–æ–≥ ~0.3)
                result["is_useful"] = bool(result.get("usefulness_score", 0.5) >= 0.3)
                return result
            else:
                # Fallback –µ—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω
                return self._fallback_result(text_truncated)

        except Exception as e:
            if self.verbose:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return self._fallback_result(text_truncated)

    def _fallback_result(self, text: str) -> Dict:
        """Fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ LLM –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        return {
            "clean_text": text,
            "products": [],
            "actions": [],
            "conditions": [],
            "topics": [],
            "usefulness_score": 0.5,
            "is_useful": True
        }

    def process_documents(self, documents_df: pd.DataFrame,
                         text_column: str = 'text') -> pd.DataFrame:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

        Args:
            documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            text_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º

        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏:
                - clean_text
                - products (JSON list)
                - actions (JSON list)
                - conditions (JSON list)
                - topics (JSON list)
                - usefulness_score (float)
                - is_useful (bool)
        """
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if self.llm is None:
            self.load_model()

        results = []

        if self.verbose:
            print(f"\nüöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(documents_df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM...")
            iterator = tqdm(documents_df.iterrows(), total=len(documents_df), desc="LLM Cleaning")
        else:
            iterator = documents_df.iterrows()

        for idx, row in iterator:
            text = row[text_column]

            # –û—á–∏—â–∞–µ–º —á–µ—Ä–µ–∑ LLM
            cleaned = self.clean_document(text)

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ + –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            result_row = {
                **row.to_dict(),
                'clean_text': cleaned.get('clean_text', text),
                'products': json.dumps(cleaned.get('products', []), ensure_ascii=False),
                'actions': json.dumps(cleaned.get('actions', []), ensure_ascii=False),
                'conditions': json.dumps(cleaned.get('conditions', []), ensure_ascii=False),
                'topics': json.dumps(cleaned.get('topics', []), ensure_ascii=False),
                'usefulness_score': cleaned.get('usefulness_score', 0.5),
                'is_useful': cleaned.get('is_useful', True)
            }

            results.append(result_row)

        result_df = pd.DataFrame(results)

        if self.verbose:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            useful_count = result_df['is_useful'].sum()
            avg_score = result_df['usefulness_score'].mean()

            print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
            print(f"   –ü–æ–ª–µ–∑–Ω—ã—Ö: {useful_count}/{len(result_df)} ({useful_count/len(result_df)*100:.1f}%)")
            print(f"   –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score:.2f}")

            # –¢–æ–ø —Ç–µ–º—ã
            all_topics = []
            for topics_json in result_df['topics']:
                try:
                    topics = json.loads(topics_json)
                    all_topics.extend(topics)
                except:
                    pass

            if all_topics:
                from collections import Counter
                topic_counts = Counter(all_topics)

                print(f"\nüìä –¢–æ–ø-5 —Ç–µ–º:")
                for topic, count in topic_counts.most_common(5):
                    print(f"   {topic}: {count}")

        return result_df

    def filter_by_usefulness(self, documents_df: pd.DataFrame,
                            min_score: float = 0.3) -> pd.DataFrame:
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏

        Args:
            documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–ø–æ—Å–ª–µ process_documents)
            min_score: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ usefulness_score

        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
        """
        before_count = len(documents_df)

        filtered_df = documents_df[
            (documents_df['is_useful'] == True) &
            (documents_df['usefulness_score'] >= min_score)
        ].copy()

        after_count = len(filtered_df)
        removed_count = before_count - after_count

        if self.verbose:
            print(f"\nüóëÔ∏è  –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (min_score={min_score}):")
            print(f"   –ë—ã–ª–æ: {before_count}")
            print(f"   –û—Å—Ç–∞–ª–æ—Å—å: {after_count}")
            print(f"   –£–¥–∞–ª–µ–Ω–æ: {removed_count} ({removed_count/before_count*100:.1f}%)")

        return filtered_df


def apply_llm_cleaning(documents_df: pd.DataFrame,
                      min_usefulness: float = 0.3,
                      verbose: bool = True) -> pd.DataFrame:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM –æ—á–∏—Å—Ç–∫–∏

    Args:
        documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        min_usefulness: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å

    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π DataFrame
    """
    cleaner = LLMDocumentCleaner(verbose=verbose)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    cleaned_df = cleaner.process_documents(documents_df)

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if min_usefulness > 0:
        cleaned_df = cleaner.filter_by_usefulness(cleaned_df, min_score=min_usefulness)

    return cleaned_df


if __name__ == "__main__":
    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
    print("="*80)
    print("–¢–ï–°–¢ LLM DOCUMENT CLEANER")
    print("="*80)

    # –¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    test_doc = """
    –ì–ª–∞–≤–Ω–∞—è / –ö–∞—Ä—Ç—ã / –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞

    –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞ - –¥–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å –∫—ç—à–±—ç–∫–æ–º

    –ü–æ–ª—É—á–∞–π—Ç–µ –∫—ç—à–±—ç–∫ 2% –Ω–∞ –≤—Å–µ –ø–æ–∫—É–ø–∫–∏ –∏ –¥–æ 10% –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –Ω–∞ –≤—ã–±–æ—Ä.
    –ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–∏ —Å—É–º–º–µ –ø–æ–∫—É–ø–æ–∫ –æ—Ç 10000 —Ä—É–±–ª–µ–π –≤ –º–µ—Å—è—Ü.

    –û—Ñ–æ—Ä–º–∏—Ç—å –æ–Ω–ª–∞–π–Ω

    ¬© 2001-2025 –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫. –õ–∏—Ü–µ–Ω–∑–∏—è –¶–ë –†–§ ‚Ññ1326
    """

    test_df = pd.DataFrame([{'text': test_doc, 'web_id': 1}])

    cleaner = LLMDocumentCleaner(verbose=True)
    result_df = cleaner.process_documents(test_df)

    print("\n" + "="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢:")
    print("="*80)
    print(f"\n–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{result_df.iloc[0]['clean_text']}")
    print(f"\n–ü—Ä–æ–¥—É–∫—Ç—ã: {result_df.iloc[0]['products']}")
    print(f"–¢–µ–º—ã: {result_df.iloc[0]['topics']}")
    print(f"–ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å: {result_df.iloc[0]['usefulness_score']}")
