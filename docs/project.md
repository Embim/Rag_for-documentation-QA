# Архитектура пайплайна: RAG + агентный RAG с оценкой полноты контекста

## 0. Цель и формат задачи

У нас есть три ключевых файла:

- **`websites.csv`** — база исходных документов:
  - `web_id` — идентификатор документа  
  - `url`, `kind`  
  - `title` — заголовок  
  - `text` — текст страницы  

- **`questions_clean.csv`** — список вопросов:
  - `q_id` — идентификатор вопроса  
  - `query` — текст вопроса  

- **`examples_for_participants.csv`** — примеры:
  - `q_id / query`  
  - `chunk_1 ... chunk_5` — идеальные фрагменты из документов  
  - `perfect_answer` — идеальный ответ  

Система **на вход** получает `websites.csv` и `questions_clean.csv`, а **на выход** должна сгенерировать файл:

- `q_id, documents_id`  
  - `documents_id` — список **до 5** `web_id`, упорядоченных по релевантности.

`examples_for_participants.csv` используется:

- как «золотой стандарт» для понимания логики,
- для обучения/тюнинга ретривера / ранкера / coverage-judge.

### Метрика

Метрика похожа на **hit@5 / recall@5**, но:

- Для каждого вопроса есть **до 5 правильных документов** (gold docs).
- Оценка для вопроса =  

\[
\text{score} = \frac{\text{число найденных правильных } web\_id \text{ в топ-5}}{\text{общее число правильных документов для этого вопроса}}
\]

- **Лишние документы не штрафуются.**  
  Если надо найти 3 документа, а ты вернул 5, из которых 3 правильные — score = 1.

Задача: **максимизировать долю найденных релевантных документов**, не упуская ни один.

---

## 1. Общий взгляд на архитектуру

Пайплайн состоит из двух больших частей:

1. **Offline-часть (индексация):**
   - Предобработка `websites.csv`
   - Чанкинг (разбиение на фрагменты)
   - Индексация в Weaviate (вектор + BM25)
   - (опционально) граф сущностей / кластеризация

2. **Online-часть (ответ на вопросы):**
   - Предобработка вопроса
   - Агентный RAG-цикл:
     - первичный поиск (hybrid: dense + BM25)
     - reranking (cross-encoder)
     - оценка полноты контекста (coverage)
     - при необходимости — дополнительные итерации поиска
   - Агрегация по `web_id` → выбор top-5 документов
   - Формирование файла `q_id, documents_id`

---

## 2. Offline-пайплайн: подготовка базы знаний (Weaviate-only, streaming)

### 2.1. Загрузка и предобработка `websites.csv`

Для каждой строки CSV берём:

- `web_id`, `title`, `text`
- метаданные: `url`, `kind`

Очистка текста:

- удаляем HTML-артефакты, футеры, меню, техвставки;
- нормализуем пробелы и спецсимволы;
- регистр не насильно понижаем (зависит от модели эмбеддингов).

LLM-clean (опционально):

- фильтрация по полезности (порог из конфигурации);
- замена `text` на `clean_text` при наличии;
- добавление метаданных: `entities`, `topics`.

Лемматизация по умолчанию выключена (заменена LLM-clean для качества).

### 2.2. Разбиение на чанки (chunking, по словам)

- Разбиение текста на фрагменты фиксированного размера по словам с перекрытием.
- Поля каждого чанка:
  - `chunk_id` — формат `web_id_chunkIndex` (например, `711_3`);
  - `web_id`, `title`, `text`, `url`, `kind`;
  - `chunk_index`, `word_count`, `char_count`;
  - опционально: `entities`, `topics`.

### 2.3. Индексация в Weaviate (стриминг)

- Эмбеддинги считаются локально (SentenceTransformers) батчами.
- Батчи чанков записываются в Weaviate сразу (вектор + свойства).
- Гибридный поиск (вектор + BM25) выполняется силами Weaviate.

Отдельные FAISS/Qdrant/Chroma не используются.

### 2.4. Классический индекс

Отдельный BM25/TF‑IDF индекс не строится: используется встроенный BM25 Weaviate в гибридном поиске.

**Результат offline-части:**

- Weaviate‑индекс: объекты с векторами и метаданными;
- Локальный файл метаданных чанков: `data/processed/chunks.pkl`.

---

## 3. Online-пайплайн без агента (базовый RAG)

### 3.1. Предобработка вопроса (`questions_clean.csv`)

Для каждого `q_id, query`:

1. Очистка:
   - убрать мусорные символы, лишние пробелы, эмодзи и т.п.
2. Нормализация (опционально):
   - лемматизатор/стеммер при необходимости,
   - словарь замен (единообразие терминов):
     - «кешбек» → «кэшбэк»,
     - «смс» → «sms» / «смс-сообщения» и т.п.

Результат: `query_clean`.

### 3.2. Query rewriting / HyDE (опционально)

Можно добавить один из вариантов:

1. **Перефразирование LLM**:
   - `query_clean = "номер счёта"` →  
     `"как узнать номер расчётного счёта в альфа-банке"`.

2. **HyDE (Hypотhetical Document Embedding)**:
   - LLM генерит гипотетический ответ:
     - "Номер счёта — это..., вы можете найти его в интернет-банке, на странице..."
   - считаем embedding этого текста,
   - используем его как вектор для sem-search.

3. Стратегия:
   - для коротких/непонятных запросов включать HyDE,
   - для нормальных — прямой embedding.

Получаем:

- `query_clean`,  
- `query_embed` — вектор (прямой или HyDE).

### 3.3. Гибридный ретривер (Weaviate hybrid: vector + BM25)

Weaviate выполняет гибридный поиск из коробки (векторный + BM25) по одному вызову API.

1. Строим эмбеддинг запроса (опционально HyDE/rewriting).
2. Вызываем `collection.query.hybrid(query=..., vector=..., alpha=...)`.
3. Получаем единый ранжированный список `candidate_chunks` (без отдельного внешнего BM25 индекса).

### 3.4. Reranking (cross-encoder)

Используем cross-encoder / reranker:

- варианты:
  - `cross-encoder/ms-marco-MiniLM-L-6-v2`,
  - `bge-reranker-base` (multi-lingual).

На вход остаётся top-N кандидатных чанков (50–100), на выходе:

- новый `rerank_score` для `(query, chunk_text)`.

Сортируем `candidate_chunks_ranked` по убыванию `rerank_score`.

### 3.5. Мэппинг чанков на документы и выбор top-5 `web_id`

Простой алгоритм:

1. Идём по `candidate_chunks_ranked` сверху вниз.
2. Для каждого chunk:
   - берём его `web_id`.
3. Если `web_id` ещё не в списке — добавляем.
4. Стоп, когда набрали 5 разных `web_id` (или кандидаты закончились).

Результат базового RAG для каждого `q_id`:

- `top5_web_ids = [w1, w2, w3, w4, w5]`.

Этот этап уже можно использовать, но он **не контролирует полноту** (coverage). Далее — агентный слой.

---

## 4. Агентный RAG с оценкой полноты контекста

Агент — это слой логики, который управляет:

- перефразированием,
- количеством вызовов ретривера,
- оценкой «хватит ли уже контекста»,
- возможно, дополнительными запросами.

### 4.1. Инструменты агента

Агент имеет набор «tools»:

1. `analyze_question(question)`  
   → разбивает вопрос на под-вопросы (subquestions).

2. `retrieve(query, k_dense, k_bm25)`  
   → гибридный ретривер Weaviate (BM25 + vector в одном запросе) + первичный скор.

3. `rerank(question, chunks)`  
   → cross-encoder, даёт точный `rerank_score`.

4. `evaluate_coverage(question, subquestions, top_chunks)`  
   → LLM-судья:
   - по каждому subquestion: `"full" / "partial" / "none"`,
   - `coverage_score ∈ [0,1]`.

5. `refine_query(question, subquestions, missing_aspects, context)`  
   → LLM формирует уточнённый запрос/запросы под недостающие аспекты.

6. (опционально) `expand_with_entities(question)`  
   → извлекает сущности (NER, например Natasha) и генерирует запросы по ним.

### 4.2. Разбиение вопросов на subquestions

`analyze_question(question)`:

- Вход: текст вопроса (нормальный человеческий).
- Выход: список под-вопросов, каждый описывает отдельный аспект.

Примеры:

**Пример 1.**  
Вопрос: «Почему не начисляется кэшбэк за оплату услуг ЖКХ?»  

Подвопросы:

1. Какие общие правила начисления кэшбэка в Альфа-Банке?
2. Входит ли оплата коммунальных услуг в категории, за которые начисляется кэшбэк?
3. Какие есть исключения или ограничения на кэшбэк по ЖКХ?

**Пример 2.**  
Вопрос: «Где узнать БИК и номер счёта?»  

Подвопросы:

1. Где в интернет-банке посмотреть БИК банка?
2. Где посмотреть номер расчётного счёта (отличный от номера карты)?

Эти subquestions используются coverage-judge’ом.

### 4.3. Coverage-judge: оценка полноты контекста

`evaluate_coverage(question, subquestions, top_chunks)`:

- Вход:
  - исходный вопрос,
  - список subquestions,
  - топ K чанков (например, 5–10), отсортированных по rerank_score.

- Задача:
  - для каждого subquestion ответить:
    - `"full"` — есть явный, достаточный ответ в тексте;
    - `"partial"` — есть намёки, но нет прямого/полного ответа;
    - `"none"` — информации по этому аспекту нет;
  - посчитать `coverage_score`, например:

\[
coverage\_score = \frac{\text{count(full)} + 0.5 \cdot \text{count(partial)}}{\text{кол-во subquestions}}
\]

- Выход (пример):

```json
{
  "sub_1": "full",
  "sub_2": "partial",
  "sub_3": "none",
  "coverage_score": 0.5
}
```

---

## 5. Быстрый старт (Weaviate)

```bash
# 1. Запустить Weaviate
docker-compose up -d

# 2. Установить зависимости
pip install -r requirements.txt

# 3. Протестировать
python scripts/test_weaviate.py

# 4. Использовать в коде
export USE_WEAVIATE=true  # по умолчанию уже true
python main_pipeline.py
```


