# ========================================
# RAG ПАЙПЛАЙН - КОНФИГУРАЦИЯ ОКРУЖЕНИЯ
# ========================================
# Скопируйте этот файл в .env и заполните необходимые значения:
#   cp .env.example .env

# ========================================
# ЛОГИРОВАНИЕ
# ========================================
LOG_LEVEL=INFO
LOG_FILE=pipeline.log

# ========================================
# LLM НАСТРОЙКИ (ОБЯЗАТЕЛЬНО!)
# ========================================
# Режим работы LLM: "local" (локальная модель через llama-cpp) или "api" (OpenRouter API)
LLM_MODE=api

# === API РЕЖИМ (OpenRouter) ===
# ВАЖНО: OpenRouter требует ключ даже для бесплатных моделей
# Получите бесплатный ключ: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# Модель для использования (примеры бесплатных моделей):
# - tngtech/deepseek-r1t2-chimera:free (бесплатно, быстро, reasoning)
# - openrouter/sherlock-think-alpha (бесплатно, reasoning, 1.8M контекст)
# - meta-llama/llama-3.2-3b-instruct:free (бесплатно, маленькая)
# Полный список: https://openrouter.ai/models
LLM_API_MODEL=tngtech/deepseek-r1t2-chimera:free

# Опционально: провайдер для роутинга (grok, openai, anthropic, и т.д.)
LLM_API_ROUTING=

# Параметры API
LLM_API_MAX_WORKERS=10
LLM_API_TIMEOUT=60
LLM_API_RETRIES=3
LLM_API_MAX_TOKENS=32768

# ========================================
# WEAVIATE (ВЕКТОРНАЯ БД)
# ========================================
USE_WEAVIATE=true
WEAVIATE_URL=http://localhost:8080

# ========================================
# ОБРАБОТКА ДАННЫХ
# ========================================
# Размер chunk'ов при чтении CSV (маленький для LLM)
CSV_CHUNKSIZE=10

# Размер chunk'ов для подсчета документов (большой для скорости)
CSV_COUNT_CHUNKSIZE=50000

# Количество параллельных воркеров для LLM обработки (1-4)
# ВНИМАНИЕ: требует много VRAM! Для A100 80GB можно 2-3
LLM_PARALLEL_WORKERS=1

# Количество параллельных воркеров для обработки вопросов (1-20)
# Рекомендуется 5-10 для API режима
QUESTION_PROCESSING_WORKERS=20

# Принудительное использование CPU вместо GPU
# FORCE_CPU=false

# ========================================
# RAG УЛУЧШЕНИЯ (ФЛАГИ)
# ========================================
# Reciprocal Rank Fusion (объединение Dense + BM25)
ENABLE_RRF=true

# Context Window (добавление соседних чанков)
ENABLE_CONTEXT_WINDOW=true

# Query Expansion (расширение запроса синонимами)
ENABLE_QUERY_EXPANSION=false

# Metadata Filtering (фильтрация по метаданным)
ENABLE_METADATA_FILTER=true

# Usefulness Filtering (фильтрация по полезности документов)
ENABLE_USEFULNESS_FILTER=true

# Dynamic TOP_K (адаптивный выбор количества результатов)
ENABLE_DYNAMIC_TOP_K=true

# ========================================
# RERANKER
# ========================================
# Тип reranker: "cross_encoder" (быстро), "llm" (качественно, медленно), "none"
RERANKER_TYPE=cross_encoder

# ========================================
# GRID SEARCH ОПТИМИЗАЦИЯ
# ========================================
# Режим grid search: "test" (5 комбинаций), "quick" (54), "full" (1225)
GRID_SEARCH_MODE=test

# Использовать LLM для оценки в grid search (точнее, но медленнее)
GRID_SEARCH_USE_LLM=true

# ========================================
# ПРОДВИНУТЫЕ ФУНКЦИИ (ОПЦИОНАЛЬНО)
# ========================================
# Parent-Child Chunking (улучшенный чанкинг)
# ENABLE_PARENT_CHILD_CHUNKING=false

# Агентный RAG (итеративный поиск)
# ENABLE_AGENT_RAG=false

# ========================================
# БЫСТРЫЙ СТАРТ
# ========================================
# 1. Получите OPENROUTER_API_KEY на https://openrouter.ai/keys
# 2. Вставьте ключ выше в OPENROUTER_API_KEY=
# 3. Запустите Weaviate: docker-compose up -d
# 4. Проверьте настройки: python main_pipeline.py check-env
# 5. Постройте базу знаний: python main_pipeline.py build
# 6. Обработайте вопросы: python main_pipeline.py search
#
# Для максимального качества:
# python main_pipeline.py all --llm-clean --optimize
#
# Документация: README.md
